{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c966a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the capital of france is paris. (score: 0.4168)\n",
      "> the capital of france is lille. (score: 0.0714)\n",
      "> the capital of france is lyon. (score: 0.0634)\n",
      "> the capital of france is marseille. (score: 0.0444)\n",
      "> the capital of france is tours. (score: 0.0303)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "# 选择模型名称，例如：bert-base-uncased、distilbert-base-uncased、roberta-base 等\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# 使用 pipeline 简化推理（例如填空任务）\n",
    "nlp = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 测试一句话：BERT 会尝试预测 [MASK] 的词\n",
    "result = nlp(\"The capital of France is [MASK].\")\n",
    "\n",
    "# 打印结果\n",
    "for r in result:\n",
    "    print(f\"> {r['sequence']} (score: {r['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0a3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "814.31s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/zsm/venv/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /home/zsm/venv/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: torch in /home/zsm/venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/zsm/venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/zsm/venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zsm/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zsm/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/zsm/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: psutil in /home/zsm/venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: setuptools in /home/zsm/venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zsm/venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/zsm/venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zsm/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zsm/venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zsm/venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zsm/venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zsm/venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zsm/venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cde99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fef1be8a8d4bc3bc7bf2c570ae87ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output= Explain the concept of black holes in simple terms: a) What are they? b) How do they form? c) What are the key features? d) What is the event horizon? e) What is Hawking radiation? f) How do we know they exist? g) What is the singularity? h) What is the accretion disk? i) What is the Schwarzschild radius? j) What are the different types of black holes?\n",
      "I need to explain black holes in simple terms, addressing multiple specific points.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# huggingface-cli login --token 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\n",
    "\n",
    "# meta-llama/Meta-Llama-3-8B-Instruct\n",
    "# 以 Meta 发布的官方模型为例（需要你登录 huggingface 并接受 license）\n",
    "# model_id = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "model_id = 'gpt2'\n",
    "\n",
    "# 加载 tokenizer 和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,     \n",
    "    offload_folder=\"./offload\",       # 把多余的参数卸载到硬盘目录\n",
    "    offload_state_dict=True,        # 开启状态字典卸载\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 使用 pipeline 进行文本生成\n",
    "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 生成文本\n",
    "prompt = \"Explain the concept of black holes in simple terms:\"\n",
    "outputs = llama_pipeline(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "\n",
    "print('output=', outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61eecba",
   "metadata": {},
   "source": [
    "Explain the concept of black holes in simple terms: a) What are they? b) How do they form? c) What are the key features? d) What is the event horizon? e) What is Hawking radiation? f) How do we know they exist? g) What is the singularity? h) What is the accretion disk? i) What is the Schwarzschild radius? j) What are the different types of black holes?\n",
    "I need to explain black holes in simple terms, addressing multiple specific points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede57aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output= [{'generated_text': 'Explain the concept of RAG (Retrieval-Augmented Generation) and its role in improving long-form content generation, and then provide an example of how you would implement RAG for generating a blog post about the future of AI in healthcare. First, explain RAG concis'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of RAG\"\n",
    "outputs = llama_pipeline(prompt, max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "print('output=', outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31a0b3",
   "metadata": {},
   "source": [
    "output= [{'generated_text': 'Explain the concept of RAG (Retrieval-Augmented Generation) and its role in improving long-form content generation, and then provide an example of how you would implement RAG for generating a blog post about the future of AI in healthcare. First, explain RAG concis'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8d70e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bab21de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsm/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/zsm/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "\n",
      "We are always looking for new clients. We want all our clients to be able to use our service in production and provide a great experience. We are looking for new clients in the following categories:\n",
      "\n",
      "1.1 - We are looking\n",
      "[{'generated_text': 'Hello world!\\n\\nWe are always looking for new clients. We want all our clients to be able to use our service in production and provide a great experience. We are looking for new clients in the following categories:\\n\\n1.1 - We are looking'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "result = pipe(\"Hello world!\", max_new_tokens=50)\n",
    "print(result[0]['generated_text'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197770cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of RAG, the concept of non-linear equations.\n",
      "\n",
      "The key to understanding RAG is to understand that RAG works in two different ways. First, there is the equation (e.g., RAG=a) that is used to explain the RAG. This equation provides the basic definitions of the RAG. Second, we can use the equation (e.g., RAG =a). This equation is used to explain the RAG in two different ways. First, we can use the equation (e.g., RAG =b) that is used to explain the RAG in two different ways. Second, we can use the equation (e.g., RAG =c) that is used to explain the RAG in two different ways.\n",
      "\n",
      "In this lecture, we will show how to use the RAG as an intuitive way to demonstrate the concept of RAG. We will also show how to use the RAG to explain a problem or a solution\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = pipe(\"Explain the concept of RAG\", max_new_tokens=200)\n",
    "print(result[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
